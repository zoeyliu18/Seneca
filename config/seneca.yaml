# seneca.yaml

## Where the vocab(s) will be written
#src_vocab: data/seneca_vocab_src_1
#tgt_vocab: data/seneca_vocab_tgt_1
src_vocab: data/seneca_vocab_src_1
tgt_vocab: data/seneca_vocab_tgt_1
# Prevent overwriting existing files in the folder
overwrite: False

# Corpus opts:
data:
    seneca:
        path_src: data/seneca_train_src_1
        path_tgt: data/seneca_train_tgt_1
    valid:
        path_src: data/seneca_dev_src_1
        path_tgt: data/seneca_dev_tgt_1


# # Model training parameters

# General opts
## Where the models will be saved
save_model: seneca_mod
## Where the samples will be written
save_data: data
keep_checkpoint: 5
save_checkpoint_steps: 5000
average_decay: 0.0005
seed: 1234
report_every: 500
train_steps: 100000
valid_steps: 5000

# Batching
batch_size: 32
valid_batch_size: 16
batch_size_multiple: 1

# Optimization
model_dtype: "fp32"
optim: "adam" #adadelta
learning_rate: 0.001
warmup_steps: 4000
decay_method: "noam"
adam_beta2: 0.998


# Model
encoder_type: brnn #transformer
decoder_type: rnn #transformer
enc_layers: 2
dec_layers: 2
#heads: 8
src_word_vec_size: 300
tgt_word_vec_size: 300
#rnn_size: 100
enc_rnn_size: 100
dec_rnn_size: 100
#rnn_type: 
word_vec_size: 300
#transformer_ff: 2048
dropout_steps: [0]
dropout: [0.1]
global_attention: mlp
attention_dropout: [0.1]
share_decoder_embeddings: true
share_embeddings: true
#position_encoding: true



...